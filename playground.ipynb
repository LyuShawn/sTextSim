{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `SimCSE`训练\n",
    "- model: BERT/RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main.trainers.cse_trainer import Trainer\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 51: 100%|██████████| 22/22 [00:02<00:00, 10.83it/s, Spearman=0.19, eval_acc=0.562, eval_loss=2.43, f1=0.715, precision=0.562, recall=0.981]\n",
      "Train: 1/15:   1%|          | 85/15625 [00:25<1:19:03,  3.28it/s, train_loss=0.248]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 16\u001b[0m\n\u001b[1;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_base_path \u001b[38;5;241m+\u001b[39m model_name_or_path)\n\u001b[1;32m      6\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m      7\u001b[0m                 from_pretrained\u001b[38;5;241m=\u001b[39mmodel_base_path \u001b[38;5;241m+\u001b[39m model_name_or_path,\n\u001b[1;32m      8\u001b[0m                 data_present_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./dataset/present.json\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 data_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWikiSTS\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     14\u001b[0m                 task_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSimCSE_Wiki_unsup\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m trainer(num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-5\u001b[39m, gpu\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m], eval_call_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     17\u001b[0m     a \u001b[38;5;241m=\u001b[39m i\n",
      "File \u001b[0;32m~/project/sTextSim/main/trainers/cse_trainer.py:123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_step, num_epochs, lr, gpu, eval_call_step, save_per_call)\u001b[0m\n\u001b[1;32m    120\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 123\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m train_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    125\u001b[0m train_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "model_base_path = './model/'\n",
    "model_name_or_path = 'bert-base-uncased'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_base_path + model_name_or_path)\n",
    "trainer = Trainer(tokenizer=tokenizer,\n",
    "                from_pretrained=model_base_path + model_name_or_path,\n",
    "                data_present_path='./dataset/present.json',\n",
    "                max_seq_len=32,\n",
    "                hard_negative_weight=0,\n",
    "                batch_size=64,\n",
    "                temp=0.05,\n",
    "                data_name='WikiSTS',\n",
    "                task_name='SimCSE_Wiki_unsup')\n",
    "\n",
    "for i in trainer(num_epochs=15, lr=2e-5, gpu=[0], eval_call_step=lambda x: x % 50 == 0):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果要单独验证请注释掉上述训练的for循环，然后运行下面的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.eval(0, is_eval=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用`SentEval`评估模型性能\n",
    "\n",
    "支持通过三种方式执行评估：\n",
    "\n",
    "- 1. 通过bash脚本执行, 其中`tokenizer_path`缺省时默认使用`model_name_or_path`指定的模型的tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!python main/evaluation.py --model_name_or_path='./model/bert-base-uncased' --tokenizer_path='./model/bert-base-uncased'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2. 通过`ipynb`执行, 其中在默认情况下将会根据`model_name_or_path`自动选择模型类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main.evaluation import *\n",
    "\n",
    "model_path = './model/bert-base-uncased'\n",
    "tokenizer_path = './model/bert-base-uncased'\n",
    "\n",
    "main([\n",
    "    '--model_name_or_path', model_path,\n",
    "    '--tokenizer_path', tokenizer_path,\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3. 也可以通过自定义`model`传入到`main()`函数来执行评估, 若采用本项目中定义的模型建议采用以下方式执行评估, 否则可能会出现丢失部分模型参数的情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main.evaluation import *\n",
    "from main.models.simcse import SimCSE\n",
    "\n",
    "model_path = './save_model/SimCSE_Wiki_unsup/simcse_best'\n",
    "tokenizer_path = './model/bert-base-uncased'\n",
    "model = SimCSE(from_pretrained=model_path,\n",
    "                                pooler_type='cls')\n",
    "\n",
    "main([\n",
    "    '--model_name_or_path', model_path,\n",
    "    '--tokenizer_path', tokenizer_path,\n",
    "], model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看保存模型的权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 指定预训练模型的路径\n",
    "model_path = 'save_model/SimCSE_Wiki_unsup/simcse_best/pytorch_model.bin'\n",
    "\n",
    "# 使用 torch.load 加载模型\n",
    "state_dict = torch.load(model_path, map_location='cpu')\n",
    "\n",
    "# 打印模型的键（参数的名称）\n",
    "print(\"Model Keys:\", state_dict.keys())\n",
    "\n",
    "# 打印模型的详细信息\n",
    "for key, value in state_dict.items():\n",
    "    print(f\"Key: {key}, Shape: {value.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `NLI`训练\n",
    "- model: BERT/SBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main.trainers.sts_trainer import Trainer\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./model/chinese_wwm_ext')\n",
    "trainer = Trainer(tokenizer=tokenizer,\n",
    "                  from_pretrained='./model/chinese_wwm_ext',\n",
    "                  model_type='bert',\n",
    "                  data_present_path='./dataset/present.json',\n",
    "                  max_seq_len=128,\n",
    "                  hard_negative_weight=0,\n",
    "                  batch_size=64,\n",
    "                  temp=0.05,\n",
    "                  data_name='CNSTS',\n",
    "                  task_name='BERT_CNSTS')\n",
    "\n",
    "for i in trainer(num_epochs=15, lr=5e-5, gpu=[0], eval_call_step=lambda x: x % 250 == 0):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果要单独验证请注释掉上述训练的for循环，然后运行下面的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.eval(0, is_eval=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simcse3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
